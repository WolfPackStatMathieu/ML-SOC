{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 09:32:10.285632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 09:32:10.594462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 09:32:10.694626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 09:32:11.501182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 09:32:18.304118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-07-31 09:32:25.417730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:af:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722418347.928244    5774 service.cc:146] XLA service 0x7f4b1c0039a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722418347.928398    5774 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-07-31 09:32:28.198497: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-31 09:32:28.963436: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8900\n",
      "2024-07-31 09:32:31.668401: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng4{k11=2} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-07-31 09:32:31.803292: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.134998516s\n",
      "Trying algorithm eng4{k11=2} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1722418352.547583    5774 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-07-31 09:32:32.808047: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng4{k11=1} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "E0000 00:00:1722418353.007274    5774 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-07-31 09:32:33.088058: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.280110308s\n",
      "Trying algorithm eng4{k11=1} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  28/2913\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7681 - loss: 0.5840  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722418354.575665    5774 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 5ms/step - accuracy: 0.9864 - loss: 0.0432 - val_accuracy: 0.9998 - val_loss: 6.3946e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 1.0427e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 1.5400e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2476e-05 - val_accuracy: 1.0000 - val_loss: 3.7984e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.0793e-07 - val_accuracy: 1.0000 - val_loss: 3.1111e-07\n",
      "Epoch 6/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6230e-07 - val_accuracy: 1.0000 - val_loss: 3.9599e-08\n",
      "Epoch 7/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.4587e-08 - val_accuracy: 1.0000 - val_loss: 1.1849e-08\n",
      "Epoch 8/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.1935e-09 - val_accuracy: 1.0000 - val_loss: 3.3644e-09\n",
      "Epoch 9/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.2922e-09 - val_accuracy: 1.0000 - val_loss: 7.7194e-10\n",
      "Epoch 10/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.9559e-10 - val_accuracy: 1.0000 - val_loss: 5.7477e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m840/840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - accuracy: 0.9845 - loss: 0.0454 - val_accuracy: 0.9996 - val_loss: 7.1475e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 1.8580e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 5.7208e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 9.9761e-04 - val_accuracy: 1.0000 - val_loss: 3.8773e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.3702e-06 - val_accuracy: 1.0000 - val_loss: 6.0701e-07\n",
      "Epoch 6/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.3869e-07 - val_accuracy: 1.0000 - val_loss: 7.6033e-08\n",
      "Epoch 7/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.0475e-08 - val_accuracy: 1.0000 - val_loss: 3.4065e-08\n",
      "Epoch 8/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.8527e-08 - val_accuracy: 1.0000 - val_loss: 7.3547e-09\n",
      "Epoch 9/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.7445e-09 - val_accuracy: 1.0000 - val_loss: 1.8804e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2562e-09 - val_accuracy: 1.0000 - val_loss: 4.0138e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m840/840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.9865 - loss: 0.0439 - val_accuracy: 0.9991 - val_loss: 0.0029\n",
      "Epoch 2/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 0.9748 - val_loss: 0.0793\n",
      "Epoch 3/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.1663e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 4.8735e-04 - val_accuracy: 1.0000 - val_loss: 5.6932e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2062e-06 - val_accuracy: 1.0000 - val_loss: 4.3000e-07\n",
      "Epoch 6/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.6137e-07 - val_accuracy: 1.0000 - val_loss: 1.0367e-07\n",
      "Epoch 7/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.4769e-08 - val_accuracy: 1.0000 - val_loss: 2.1777e-08\n",
      "Epoch 8/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.1643e-08 - val_accuracy: 1.0000 - val_loss: 4.3494e-09\n",
      "Epoch 9/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.1092e-09 - val_accuracy: 1.0000 - val_loss: 4.0985e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0558e-09 - val_accuracy: 1.0000 - val_loss: 6.8584e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m840/840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2911/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9861 - loss: 0.0448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 09:39:04.114248: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng3{k11=2} for conv (f32[128,128,1,5]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,45,1,200]{3,2,1,0}, f32[128,45,1,196]{3,2,1,0}), window={size=1x196}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-07-31 09:39:04.500054: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.385932161s\n",
      "Trying algorithm eng3{k11=2} for conv (f32[128,128,1,5]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,45,1,200]{3,2,1,0}, f32[128,45,1,196]{3,2,1,0}), window={size=1x196}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - accuracy: 0.9861 - loss: 0.0448 - val_accuracy: 0.9988 - val_loss: 0.0030\n",
      "Epoch 2/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 8.8136e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.0601e-04 - val_accuracy: 1.0000 - val_loss: 1.6619e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 8.1077e-04 - val_accuracy: 1.0000 - val_loss: 7.2689e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 4.6936e-04 - val_accuracy: 1.0000 - val_loss: 6.1558e-06\n",
      "Epoch 6/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6462e-04 - val_accuracy: 1.0000 - val_loss: 2.3644e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 5.4567e-04 - val_accuracy: 1.0000 - val_loss: 9.7339e-07\n",
      "Epoch 8/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 7.2913e-07 - val_accuracy: 1.0000 - val_loss: 1.9745e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.5571e-07 - val_accuracy: 1.0000 - val_loss: 3.8030e-08\n",
      "Epoch 10/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 3.4962e-08 - val_accuracy: 1.0000 - val_loss: 1.0629e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m840/840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.9862 - loss: 0.0434 - val_accuracy: 0.9987 - val_loss: 0.0026\n",
      "Epoch 2/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9995 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 4.0922e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 2.1427e-04 - val_accuracy: 1.0000 - val_loss: 9.9696e-07\n",
      "Epoch 4/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 6.9585e-07 - val_accuracy: 1.0000 - val_loss: 6.4133e-07\n",
      "Epoch 5/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.2636e-07 - val_accuracy: 1.0000 - val_loss: 6.1681e-08\n",
      "Epoch 6/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 9.4951e-04 - val_accuracy: 0.9998 - val_loss: 3.3812e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 4.4861e-04 - val_accuracy: 1.0000 - val_loss: 2.6493e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 5.1330e-04 - val_accuracy: 1.0000 - val_loss: 5.5687e-06\n",
      "Epoch 9/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.7339e-04 - val_accuracy: 1.0000 - val_loss: 1.4287e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3948e-04 - val_accuracy: 1.0000 - val_loss: 1.9357e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m840/840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Mean accuracy: 0.9997244238853454, Standard Deviation: 0.00014441433032592302\n",
      "INFO:root:Mean precision: 0.9979146718978882, Standard Deviation: 0.0009305017883889377\n",
      "INFO:root:Mean recall: 0.9943879842758179, Standard Deviation: 0.0037541105411946774\n",
      "INFO:root:Mean F1-score: 0.9930062294006348, Standard Deviation: 0.0035828612744808197\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def load_partial_data(badqueries_file, goodqueries_file, fraction=0.1):\n",
    "    \"\"\"\n",
    "    Charge un extrait proportionnel des fichiers de requêtes mauvaises et bonnes.\n",
    "    \n",
    "    Args:\n",
    "    badqueries_file (str): Chemin vers le fichier contenant les mauvaises requêtes.\n",
    "    goodqueries_file (str): Chemin vers le fichier contenant les bonnes requêtes.\n",
    "    fraction (float): Fraction des données à charger.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tableaux numpy des URL et des labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def read_lines(file_path):\n",
    "        \"\"\"\n",
    "        Lit un fichier ligne par ligne.\n",
    "        \n",
    "        Args:\n",
    "        file_path (str): Chemin vers le fichier à lire.\n",
    "        \n",
    "        Returns:\n",
    "        list: Liste des lignes du fichier.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        return lines\n",
    "    \n",
    "    # Lecture des fichiers ligne par ligne\n",
    "    bad_queries = read_lines(badqueries_file)\n",
    "    good_queries = read_lines(goodqueries_file)\n",
    "    \n",
    "    # Prendre un échantillon proportionnel\n",
    "    bad_queries_sample = np.random.choice(bad_queries, size=int(len(bad_queries) * fraction), replace=False)\n",
    "    good_queries_sample = np.random.choice(good_queries, size=int(len(good_queries) * fraction), replace=False)\n",
    "    \n",
    "    # Ajouter les étiquettes et créer des DataFrames\n",
    "    bad_queries_sample = pd.DataFrame(bad_queries_sample, columns=[\"URL\"])\n",
    "    bad_queries_sample['label'] = 1\n",
    "    good_queries_sample = pd.DataFrame(good_queries_sample, columns=[\"URL\"])\n",
    "    good_queries_sample['label'] = 0\n",
    "    \n",
    "    # Combiner et mélanger les données\n",
    "    data = pd.concat([bad_queries_sample, good_queries_sample])\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)  # Mélanger les données\n",
    "    \n",
    "    return data['URL'].values, data['label'].values\n",
    "\n",
    "def preprocess_data(urls):\n",
    "    \"\"\"\n",
    "    Convertit les URL en séquences ASCII avec une longueur maximale de 200 caractères.\n",
    "    \n",
    "    Args:\n",
    "    urls (array): Tableau d'URL à convertir.\n",
    "    \n",
    "    Returns:\n",
    "    array: Tableau numpy des séquences ASCII.\n",
    "    \"\"\"\n",
    "    max_length = 200  # Troncature ou padding des séquences à une longueur maximale de 200\n",
    "    ascii_data = np.zeros((len(urls), max_length), dtype=int)\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        ascii_values = [ord(char) if ord(char) < 128 else 127 for char in url[:max_length]]\n",
    "        ascii_data[i, :len(ascii_values)] = ascii_values\n",
    "    \n",
    "    return ascii_data\n",
    "\n",
    "def create_cnn_model(input_length):\n",
    "    \"\"\"\n",
    "    Crée un modèle de réseau de neurones convolutionnel (CNN) pour la classification.\n",
    "    \n",
    "    Args:\n",
    "    input_length (int): Longueur des séquences d'entrée.\n",
    "    \n",
    "    Returns:\n",
    "    model: Modèle CNN compilé.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=128, output_dim=128, input_length=input_length),\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    \"\"\"\n",
    "    Équilibre les classes dans les données d'entraînement par sur-échantillonnage.\n",
    "    \n",
    "    Args:\n",
    "    X (array): Données d'entrée.\n",
    "    y (array): Étiquettes correspondantes.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Données et étiquettes équilibrées.\n",
    "    \"\"\"\n",
    "    # Séparer les classes\n",
    "    X_neg = X[y == 0]\n",
    "    y_neg = y[y == 0]\n",
    "    X_pos = X[y == 1]\n",
    "    y_pos = y[y == 1]\n",
    "    \n",
    "    # Équilibrer les classes par sur-échantillonnage\n",
    "    X_pos, y_pos = resample(X_pos, y_pos, replace=True, n_samples=len(X_neg), random_state=42)\n",
    "    \n",
    "    # Combiner les données équilibrées\n",
    "    X_balanced = np.vstack((X_neg, X_pos))\n",
    "    y_balanced = np.concatenate((y_neg, y_pos))\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Charger les données avec une fraction de 10%\n",
    "X, y = load_partial_data('badqueries.txt', 'goodqueries.txt', fraction=0.1)\n",
    "\n",
    "# Initialiser StratifiedKFold pour la validation croisée\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Listes pour stocker les métriques de performance\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "errors = []\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Prétraiter les données en convertissant les URL en séquences ASCII\n",
    "    X_train = preprocess_data(X_train)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    \n",
    "    # Équilibrer les classes dans les données d'entraînement\n",
    "    X_train_balanced, y_train_balanced = balance_classes(X_train, y_train)\n",
    "    \n",
    "    # Créer le modèle CNN\n",
    "    model = create_cnn_model(input_length=X_train_balanced.shape[1])\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train_balanced, y_train_balanced, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "    \n",
    "    # Sauvegarder le modèle pour ce pli\n",
    "    model.save(f'model_fold_{fold}.h5')\n",
    "    \n",
    "    # Évaluer le modèle sur les données de test\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracies.append(scores[1])\n",
    "    \n",
    "    # Prédictions sur les données de test\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    \n",
    "    # Enregistrer les erreurs\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != y_pred[i]:\n",
    "            errors.append((X[test_index[i]], y_test[i], y_pred[i]))\n",
    "    \n",
    "    # Calcul des métriques de performance\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    f1_score = tf.keras.metrics.AUC(curve='PR')\n",
    "    \n",
    "    precision.update_state(y_test, y_pred)\n",
    "    recall.update_state(y_test, y_pred)\n",
    "    f1_score.update_state(y_test, y_pred)\n",
    "    \n",
    "    precisions.append(precision.result().numpy())\n",
    "    recalls.append(recall.result().numpy())\n",
    "    f1_scores.append(f1_score.result().numpy())\n",
    "\n",
    "# Calculer les moyennes et écarts-types des métriques\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "# Afficher les résultats\n",
    "logging.info(f'Mean accuracy: {mean_accuracy}, Standard Deviation: {std_accuracy}')\n",
    "logging.info(f'Mean precision: {mean_precision}, Standard Deviation: {std_precision}')\n",
    "logging.info(f'Mean recall: {mean_recall}, Standard Deviation: {std_recall}')\n",
    "logging.info(f'Mean F1-score: {mean_f1_score}, Standard Deviation: {std_f1_score}')\n",
    "\n",
    "# Enregistrer les erreurs dans un fichier CSV\n",
    "with open('errors.csv', 'w', newline='') as csvfile:\n",
    "    error_writer = csv.writer(csvfile)\n",
    "    error_writer.writerow(['URL', 'True Label', 'Predicted Label'])\n",
    "    for error in errors:\n",
    "        url, true_label, pred_label = error\n",
    "        error_writer.writerow([url, true_label, pred_label])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
