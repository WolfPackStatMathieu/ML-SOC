{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 14:09:32.360575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 14:09:32.394437: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 14:09:32.404651: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 14:09:32.433681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 14:09:33.850022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-07-30 14:09:37.422052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:af:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722348579.994777   23673 service.cc:146] XLA service 0x7fda140069f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722348579.994862   23673 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-07-30 14:09:40.055604: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-30 14:09:40.311568: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8900\n",
      "2024-07-30 14:09:41.880712: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng4{k11=2} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-07-30 14:09:41.995383: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.114761925s\n",
      "Trying algorithm eng4{k11=2} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1722348582.819244   23673 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-07-30 14:09:43.000514: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng4{k11=1} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "E0000 00:00:1722348583.321689   23673 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-07-30 14:09:43.385761: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.385351275s\n",
      "Trying algorithm eng4{k11=1} for conv (f32[64,128,1,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,128,1,196]{3,2,1,0}, f32[128,128,1,5]{3,2,1,0}), window={size=1x5}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  42/2913\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8222 - loss: 0.5356 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722348584.495859   23673 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2910/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9874 - loss: 0.0431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 14:09:56.892886: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng3{k11=2} for conv (f32[128,128,1,5]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,43,1,200]{3,2,1,0}, f32[128,43,1,196]{3,2,1,0}), window={size=1x196}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-07-30 14:09:57.101920: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.209212157s\n",
      "Trying algorithm eng3{k11=2} for conv (f32[128,128,1,5]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,43,1,200]{3,2,1,0}, f32[128,43,1,196]{3,2,1,0}), window={size=1x196}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2913/2913\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0431"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def load_partial_data(badqueries_file, goodqueries_file, fraction=0.1):\n",
    "    \"\"\"\n",
    "    Charge un extrait proportionnel des fichiers de requêtes mauvaises et bonnes.\n",
    "    \n",
    "    Args:\n",
    "    badqueries_file (str): Chemin vers le fichier contenant les mauvaises requêtes.\n",
    "    goodqueries_file (str): Chemin vers le fichier contenant les bonnes requêtes.\n",
    "    fraction (float): Fraction des données à charger.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tableaux numpy des URL et des labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def read_lines(file_path):\n",
    "        \"\"\"\n",
    "        Lit un fichier ligne par ligne.\n",
    "        \n",
    "        Args:\n",
    "        file_path (str): Chemin vers le fichier à lire.\n",
    "        \n",
    "        Returns:\n",
    "        list: Liste des lignes du fichier.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        return lines\n",
    "    \n",
    "    # Lecture des fichiers ligne par ligne\n",
    "    bad_queries = read_lines(badqueries_file)\n",
    "    good_queries = read_lines(goodqueries_file)\n",
    "    \n",
    "    # Prendre un échantillon proportionnel\n",
    "    bad_queries_sample = np.random.choice(bad_queries, size=int(len(bad_queries) * fraction), replace=False)\n",
    "    good_queries_sample = np.random.choice(good_queries, size=int(len(good_queries) * fraction), replace=False)\n",
    "    \n",
    "    # Ajouter les étiquettes et créer des DataFrames\n",
    "    bad_queries_sample = pd.DataFrame(bad_queries_sample, columns=[\"URL\"])\n",
    "    bad_queries_sample['label'] = 1\n",
    "    good_queries_sample = pd.DataFrame(good_queries_sample, columns=[\"URL\"])\n",
    "    good_queries_sample['label'] = 0\n",
    "    \n",
    "    # Combiner et mélanger les données\n",
    "    data = pd.concat([bad_queries_sample, good_queries_sample])\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)  # Mélanger les données\n",
    "    \n",
    "    return data['URL'].values, data['label'].values\n",
    "\n",
    "def preprocess_data(urls):\n",
    "    \"\"\"\n",
    "    Convertit les URL en séquences ASCII avec une longueur maximale de 200 caractères.\n",
    "    \n",
    "    Args:\n",
    "    urls (array): Tableau d'URL à convertir.\n",
    "    \n",
    "    Returns:\n",
    "    array: Tableau numpy des séquences ASCII.\n",
    "    \"\"\"\n",
    "    max_length = 200  # Troncature ou padding des séquences à une longueur maximale de 200\n",
    "    ascii_data = np.zeros((len(urls), max_length), dtype=int)\n",
    "    \n",
    "    for i, url in enumerate(urls):\n",
    "        ascii_values = [ord(char) if ord(char) < 128 else 127 for char in url[:max_length]]\n",
    "        ascii_data[i, :len(ascii_values)] = ascii_values\n",
    "    \n",
    "    return ascii_data\n",
    "\n",
    "def create_cnn_model(input_length):\n",
    "    \"\"\"\n",
    "    Crée un modèle de réseau de neurones convolutionnel (CNN) pour la classification.\n",
    "    \n",
    "    Args:\n",
    "    input_length (int): Longueur des séquences d'entrée.\n",
    "    \n",
    "    Returns:\n",
    "    model: Modèle CNN compilé.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=128, output_dim=128, input_length=input_length),\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    \"\"\"\n",
    "    Équilibre les classes dans les données d'entraînement par sur-échantillonnage.\n",
    "    \n",
    "    Args:\n",
    "    X (array): Données d'entrée.\n",
    "    y (array): Étiquettes correspondantes.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Données et étiquettes équilibrées.\n",
    "    \"\"\"\n",
    "    # Séparer les classes\n",
    "    X_neg = X[y == 0]\n",
    "    y_neg = y[y == 0]\n",
    "    X_pos = X[y == 1]\n",
    "    y_pos = y[y == 1]\n",
    "    \n",
    "    # Équilibrer les classes par sur-échantillonnage\n",
    "    X_pos, y_pos = resample(X_pos, y_pos, replace=True, n_samples=len(X_neg), random_state=42)\n",
    "    \n",
    "    # Combiner les données équilibrées\n",
    "    X_balanced = np.vstack((X_neg, X_pos))\n",
    "    y_balanced = np.concatenate((y_neg, y_pos))\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Charger les données avec une fraction de 10%\n",
    "X, y = load_partial_data('badqueries.txt', 'goodqueries.txt', fraction=0.1)\n",
    "\n",
    "# Initialiser StratifiedKFold pour la validation croisée\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Listes pour stocker les métriques de performance\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "errors = []\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Prétraiter les données en convertissant les URL en séquences ASCII\n",
    "    X_train = preprocess_data(X_train)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    \n",
    "    # Équilibrer les classes dans les données d'entraînement\n",
    "    X_train_balanced, y_train_balanced = balance_classes(X_train, y_train)\n",
    "    \n",
    "    # Créer le modèle CNN\n",
    "    model = create_cnn_model(input_length=X_train_balanced.shape[1])\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train_balanced, y_train_balanced, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "    \n",
    "    # Évaluer le modèle sur les données de test\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracies.append(scores[1])\n",
    "    \n",
    "    # Prédictions sur les données de test\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    \n",
    "    # Enregistrer les erreurs\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] != y_pred[i]:\n",
    "            errors.append((X[test_index[i]], y_test[i], y_pred[i]))\n",
    "    \n",
    "    # Calcul des métriques de performance\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    f1_score = tf.keras.metrics.AUC(curve='PR')\n",
    "    \n",
    "    precision.update_state(y_test, y_pred)\n",
    "    recall.update_state(y_test, y_pred)\n",
    "    f1_score.update_state(y_test, y_pred)\n",
    "    \n",
    "    precisions.append(precision.result().numpy())\n",
    "    recalls.append(recall.result().numpy())\n",
    "    f1_scores.append(f1_score.result().numpy())\n",
    "\n",
    "# Calculer les moyennes et écarts-types des métriques\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "std_f1_score = np.std(f1_scores)\n",
    "\n",
    "# Afficher les résultats\n",
    "logging.info(f'Mean accuracy: {mean_accuracy}, Standard Deviation: {std_accuracy}')\n",
    "logging.info(f'Mean precision: {mean_precision}, Standard Deviation: {std_precision}')\n",
    "logging.info(f'Mean recall: {mean_recall}, Standard Deviation: {std_recall}')\n",
    "logging.info(f'Mean F1-score: {mean_f1_score}, Standard Deviation: {std_f1_score}')\n",
    "\n",
    "# Enregistrer les erreurs dans un fichier CSV\n",
    "with open('errors.csv', 'w', newline='') as csvfile:\n",
    "    error_writer = csv.writer(csvfile)\n",
    "    error_writer.writerow(['URL', 'True Label', 'Predicted Label'])\n",
    "    for error in errors:\n",
    "        url, true_label, pred_label = error\n",
    "        error_writer.writerow([url, true_label, pred_label])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
